{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import model_selection\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              tweet  class\n",
       "0           0  !!! RT @mayasolovely: As a woman you shouldn't...      0\n",
       "1           1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...      1\n",
       "2           2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...      1\n",
       "3           3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...      1\n",
       "4           4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...      1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\Sahana\\Aggression-Detection\\dataset\\raw_data.csv')\n",
    "nan_value = float(\"NaN\")\n",
    "df.replace(\"\", nan_value, inplace=True)\n",
    "#drop rows with NaN value\n",
    "df.dropna(subset = [\"tweet\"], inplace=True)\n",
    "df.drop_duplicates()\n",
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing c:\\users\\sahana\\tweet-preprocessor-0.4.0\n",
      "Building wheels for collected packages: tweet-preprocessor\n",
      "  Building wheel for tweet-preprocessor (setup.py): started\n",
      "  Building wheel for tweet-preprocessor (setup.py): finished with status 'done'\n",
      "  Created wheel for tweet-preprocessor: filename=tweet_preprocessor-0.4.0-py3-none-any.whl size=7573 sha256=8a5c47d110920bc3ec6d521eb55ed540ab80fb80bea841b5c5ff66fc586ae73e\n",
      "  Stored in directory: c:\\users\\sahana\\appdata\\local\\pip\\cache\\wheels\\a9\\34\\79\\bb0aefe11348ea0bc31bffb8186b9e13ef58788379f51b9528\n",
      "Successfully built tweet-preprocessor\n",
      "Installing collected packages: tweet-preprocessor\n",
      "Successfully installed tweet-preprocessor-0.4.0\n"
     ]
    }
   ],
   "source": [
    "#basic pre-processing using tweet-preprocessor\n",
    "'''\n",
    "removes :\n",
    "URLs\n",
    "Hashtags\n",
    "Mentions\n",
    "Reserved words (RT, FAV)\n",
    "Emojis\n",
    "Smileys\n",
    "'''\n",
    "!pip  install \"C:\\Users\\Sahana\\tweet-preprocessor-0.4.0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessor as p\n",
    "def tweet_pro(row):\n",
    "    tweet = row['tweet']\n",
    "    tweet = p.clean(tweet)\n",
    "    tweet = tweet.lower()\n",
    "    tweet = tweet.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    return tweet\n",
    "\n",
    "df['tweet'] = df.apply(tweet_pro,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.to_csv(\"raw_data1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/0a/19/2b2c0e1340131a8e23ce4a9804cdccdd62d4d23d3d86c1754857b3de7a14/spacy-2.2.4-cp36-cp36m-win_amd64.whl\n",
      "Collecting wasabi<1.1.0,>=0.4.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/21/e1/e4e7b754e6be3a79c400eb766fb34924a6d278c43bb828f94233e0124a21/wasabi-0.6.0-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy) (41.0.1)\n",
      "Collecting blis<0.5.0,>=0.4.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/7b/98/00e345edf2ef6d66a8c3cd08779a9829f13c76b37bf3c2445e9965881c2f/blis-0.4.1-cp36-cp36m-win_amd64.whl\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/15/8f/0dad3ca706e31258cf7b9adf40f8d2103444a09dd7d66d46cf6980025c65/murmurhash-1.0.2-cp36-cp36m-win_amd64.whl\n",
      "Collecting plac<1.2.0,>=0.9.6 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/b0/71/a58322c3489bf0f5a71aa69a66b42164cbc4f0d5ac5e1042c11233766b3f/preshed-3.0.2-cp36-cp36m-win_amd64.whl\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/47/55/fd9170ba08a1a64a18a7f8a18f088037316f2a41be04d2fe6ece5a653e8f/tqdm-4.43.0-py2.py3-none-any.whl\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/dd/ec/904b4741879a2a280a40d5bf0b61449a20d1f75281e14ebee06566f7765b/cymem-2.0.3-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy) (2.22.0)\n",
      "Collecting thinc==7.4.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/dd/f5/4c76b84a9ae0ea6c659285cf8fed8cce76d5db5b8353e1e08b8d3f56058e/thinc-7.4.0-cp36-cp36m-win_amd64.whl\n",
      "Collecting catalogue<1.1.0,>=0.0.7 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy) (1.18.1)\n",
      "Collecting srsly<1.1.0,>=1.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/3a/d6/939d46c05289b185226a576421079468123b1719ffe16e181e0005d45ef9/srsly-1.0.2-cp36-cp36m-win_amd64.whl\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.2)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Collecting importlib-metadata>=0.20; python_version < \"3.8\" (from catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/0f/b0/3302e1a1711aaa920e82b025ef07fb5fe81f09634e4f7af0fb4695b72cab/importlib_metadata-1.5.2-py2.py3-none-any.whl\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
      "Installing collected packages: wasabi, blis, murmurhash, plac, cymem, preshed, tqdm, srsly, zipp, importlib-metadata, catalogue, thinc, spacy\n",
      "  Found existing installation: tqdm 4.32.1\n",
      "    Uninstalling tqdm-4.32.1:\n",
      "      Successfully uninstalled tqdm-4.32.1\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 importlib-metadata-1.5.2 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 spacy-2.2.4 srsly-1.0.2 thinc-7.4.0 tqdm-4.43.0 wasabi-0.6.0 zipp-3.1.0\n",
      "Collecting en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0MB)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.43.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.22.0)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (41.0.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.2)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-cp36-none-any.whl size=12011744 sha256=066cebb75cb966dee838376e9a4510ccd3b17497f84a73470e7a6b46ec1cf6f4\n",
      "  Stored in directory: C:\\Users\\Sahana\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-i6ql59hn\\wheels\\6a\\47\\fb\\6b5a0b8906d8e8779246c67d4658fd8a544d4a03a75520197a\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-lookups-data\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/f1/be61b032e02a06a221e14f906dc251de90ac459dc2739f0c5225844ecb08/spacy_lookups_data-0.2.0.tar.gz\n",
      "Requirement already satisfied, skipping upgrade: setuptools in c:\\users\\sahana\\anaconda3\\lib\\site-packages (from spacy-lookups-data) (41.0.1)\n",
      "Building wheels for collected packages: spacy-lookups-data\n",
      "  Building wheel for spacy-lookups-data (setup.py): started\n",
      "  Building wheel for spacy-lookups-data (setup.py): finished with status 'done'\n",
      "  Created wheel for spacy-lookups-data: filename=spacy_lookups_data-0.2.0-py2.py3-none-any.whl size=29164788 sha256=2ba26fd3cc29b888fdcd19461a69e7ed7be65e1c3624c522e38f7bc990c4c577\n",
      "  Stored in directory: C:\\Users\\Sahana\\AppData\\Local\\pip\\Cache\\wheels\\79\\a4\\b8\\6085d282396938b29675292697e72871b145990d0079ceadc1\n",
      "Successfully built spacy-lookups-data\n",
      "Installing collected packages: spacy-lookups-data\n",
      "Successfully installed spacy-lookups-data-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install -U spacy-lookups-data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete empty rows (after pre-processing)\n",
    "\n",
    "df = pd.read_csv(r'C:\\Users\\Sahana\\Desktop\\final year project\\final pro\\raw_data1.csv')\n",
    "nan_value = float(\"NaN\")\n",
    "df.replace(\"\", nan_value, inplace=True)\n",
    "#drop rows with NaN value\n",
    "df.dropna(axis = 0, subset = ['tweet'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization,lemmatization using spaCy\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "\n",
    "def lemmatize(doc):\n",
    "    doc = doc['tweet']\n",
    "    doc = nlp(doc)\n",
    "    tokens = [tokens for tokens in doc if ((tokens.is_stop == False) and tokens.text != '')]\n",
    "    lemma = [token.lemma_ for token in tokens]\n",
    "    \n",
    "    return \" \".join(lemma)\n",
    "\n",
    "df['lemma_text'] = df.apply(lemmatize,axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_x, test_x, train_y, test_y = model_selection.train_test_split(df['lemma_text'], df['class'],test_size=0.33, random_state=42)\n",
    "print(train_x.shape,test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../paths.py\n",
    "CONST = CONST()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.read_csv(CONST.CLEANED_TRAIN).tweet\n",
    "test_x = pd.read_csv(CONST.CLEANED_TEST).tweet\n",
    "train_y = pd.read_csv(CONST.CLEANED_TRAIN)[\"class\"]\n",
    "test_y = pd.read_csv(CONST.CLEANED_TEST)[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature vectors using tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "#word unigrams,bigrams,trigramsma\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1,2),max_features=10000).fit(train_x)\n",
    "\n",
    "\n",
    "\n",
    "#feature vectors\n",
    "x = word_vectorizer.transform(train_x)\n",
    "\n",
    "#testing feature vectors\n",
    "x_test = word_vectorizer.transform(test_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42210, 10000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature vectors using tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "#word unigrams,bigrams,trigrams\n",
    "word_vectorizer = TfidfVectorizer(analyzer = 'word',token_pattern = r'\\w+',ngram_range=(1,3)).fit(train_x)\n",
    "\n",
    "#character n-grams\n",
    "char_vectorizer =  TfidfVectorizer(analyzer = 'char',token_pattern = r'\\w+',ngram_range=(2,8)).fit(train_x)\n",
    "\n",
    "#combine\n",
    "vectorizer = FeatureUnion([\n",
    "('word_vectorizer', TfidfVectorizer(analyzer = 'word',token_pattern = r'\\w+',ngram_range=(1,3))),\n",
    "('char_vectorizer', TfidfVectorizer(analyzer = 'char',token_pattern = r'\\w+',ngram_range=(2,8)))]).fit(train_x)\n",
    "\n",
    "\n",
    "#training feature vectors\n",
    "x = vectorizer.transform(train_x)\n",
    "word_x = word_vectorizer.transform(train_x)\n",
    "char_x = char_vectorizer.transform(train_x)\n",
    "\n",
    "#testing feature vectors\n",
    "x_test = vectorizer.transform(test_x)\n",
    "word_x_test = word_vectorizer.transform(test_x)\n",
    "char_x_test = char_vectorizer.transform(test_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307267\n",
      "1692965\n"
     ]
    }
   ],
   "source": [
    "#no. of features/dimensions\n",
    "print(len(word_vectorizer.vocabulary_))\n",
    "print(len(char_vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete rows if lemma_text is empty\n",
    "\n",
    "nan_value = float(\"NaN\")\n",
    "df.replace(\"\", nan_value, inplace=True)\n",
    "#drop rows with NaN value\n",
    "df.dropna(axis = 0, subset = ['lemma_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56484)\t0.17169391165258482\n",
      "  (0, 56499)\t0.2559170011632014\n",
      "  (0, 56500)\t0.2559170011632014\n",
      "  (0, 61434)\t0.17966464498520657\n",
      "  (0, 61451)\t0.2559170011632014\n",
      "  (0, 61452)\t0.2559170011632014\n",
      "  (0, 90427)\t0.2298918358275759\n",
      "  (0, 114515)\t0.2559170011632014\n",
      "  (0, 114516)\t0.2559170011632014\n",
      "  (0, 114517)\t0.2559170011632014\n",
      "  (0, 136372)\t0.2342108742260666\n",
      "  (0, 136373)\t0.2559170011632014\n",
      "  (0, 136374)\t0.2559170011632014\n",
      "  (0, 290170)\t0.22028672164381308\n",
      "  (0, 290173)\t0.2298918358275759\n",
      "  (0, 303492)\t0.09344358735132607\n",
      "  (0, 303906)\t0.2559170011632014\n",
      "  (0, 303907)\t0.2559170011632014\n",
      "  (0, 338397)\t0.033794824008096185\n",
      "  (0, 342031)\t0.03350183016004224\n",
      "  (0, 342310)\t0.04500450194505963\n",
      "  (0, 342356)\t0.050611978222283915\n",
      "  (0, 342361)\t0.05137020168108364\n",
      "  (0, 342362)\t0.05147093528554256\n",
      "  (0, 342369)\t0.07671959522941506\n",
      "  :\t:\n",
      "  (0, 1845103)\t0.06529001088239018\n",
      "  (0, 1845104)\t0.06529001088239018\n",
      "  (0, 1845108)\t0.06891769015373583\n",
      "  (0, 1845109)\t0.06891769015373583\n",
      "  (0, 1911771)\t0.04353503378092913\n",
      "  (0, 1911772)\t0.04353503378092913\n",
      "  (0, 1911976)\t0.06782297367935364\n",
      "  (0, 1911977)\t0.06891769015373583\n",
      "  (0, 1911981)\t0.07671959522941506\n",
      "  (0, 1911982)\t0.07671959522941506\n",
      "  (0, 1911983)\t0.07671959522941506\n",
      "  (0, 1946980)\t0.04155034334908284\n",
      "  (0, 1947312)\t0.06782297367935364\n",
      "  (0, 1947313)\t0.06891769015373583\n",
      "  (0, 1947318)\t0.07671959522941506\n",
      "  (0, 1947319)\t0.07671959522941506\n",
      "  (0, 1947320)\t0.07671959522941506\n",
      "  (0, 1947321)\t0.07671959522941506\n",
      "  (0, 1982677)\t0.04337101725176645\n",
      "  (0, 1982697)\t0.04353503378092913\n",
      "  (0, 1982698)\t0.04353503378092913\n",
      "  (0, 1982835)\t0.06782297367935364\n",
      "  (0, 1982836)\t0.06891769015373583\n",
      "  (0, 1982839)\t0.07671959522941506\n",
      "  (0, 1982840)\t0.07671959522941506\n"
     ]
    }
   ],
   "source": [
    "#an example of a feature vector\n",
    "print(x[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "from sklearn import svm\n",
    "#word n-grams \n",
    "word_clf = svm.SVC(gamma ='auto').fit(word_x,train_y)\n",
    "word_clf_pre = svm.SVC(gamma = 'auto', kernel = 'linear').fit(word_x,train_y)\n",
    "word_clf_poly = svm.SVC(gamma = 'auto', kernel = 'poly').fit(word_x,train_y)\n",
    "\n",
    "#char n-grams\n",
    "char_clf = svm.SVC(gamma ='auto').fit(char_x,train_y)\n",
    "char_clf_pre = svm.SVC(gamma = 'auto', kernel = 'linear').fit(char_x,train_y)\n",
    "char_clf_poly = svm.SVC(gamma = 'auto', kernel = 'poly').fit(char_x,train_y)\n",
    "\n",
    "\n",
    "#combined fea.\n",
    "clf = svm.SVC(gamma ='auto').fit(x,train_y)\n",
    "clf_pre = svm.SVC(gamma = 'auto', kernel = 'linear').fit(x,train_y)\n",
    "clf_poly = svm.SVC(gamma = 'auto', kernel = 'poly').fit(x,train_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "from sklearn import svm\n",
    "\n",
    "#word n-grams \n",
    "word_clf = svm.SVC(gamma ='auto').fit(x,train_y)\n",
    "word_clf_pre = svm.SVC(gamma = 'auto', kernel = 'linear').fit(x,train_y)\n",
    "word_clf_poly = svm.SVC(gamma = 'auto', kernel = 'poly').fit(x,train_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with word n-gram features .... \n",
      "linear kernel \n",
      "accuracy ->  94.21494542401344\n",
      "F1 score -> 0.9417339926458201\n",
      "rbf kernel\n",
      "accuracy ->  58.211586901763226\n",
      "F1 score -> 0.4283616536537966\n",
      "Polynomial kernel\n",
      "accuracy ->  58.211586901763226\n",
      "F1 score -> 0.4283616536537966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#word n-grams\n",
    "\n",
    "word_pre = word_clf.predict(x_test)\n",
    "word_pre_pre= word_clf_pre.predict(x_test)\n",
    "word_pre_poly= word_clf_poly.predict(x_test)\n",
    "print('Prediction with word n-gram features .... ')\n",
    "print(\"linear kernel \")\n",
    "print(\"accuracy -> \",accuracy_score(word_pre_pre, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,word_pre_pre,average='weighted'))\n",
    "print(\"rbf kernel\")\n",
    "print(\"accuracy -> \",accuracy_score(word_pre, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,word_pre,average='weighted'))\n",
    "print(\"Polynomial kernel\")\n",
    "print(\"accuracy -> \",accuracy_score(word_pre_poly, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,word_pre_poly,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with word n-gram features .... \n",
      "linear kernel \n",
      "accuracy ->  94.13052017282982\n",
      "F1 score -> 0.9411520366044245\n",
      "rbf kernel\n",
      "accuracy ->  58.08316031648055\n",
      "F1 score -> 0.42682009969891394\n",
      "Polynomial kernel\n",
      "accuracy ->  58.08316031648055\n",
      "F1 score -> 0.42682009969891394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sahana\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with char n-gram features .... \n",
      "linear kernel \n",
      "accuracy ->  94.45597890129622\n",
      "F1 score -> 0.9443279990037949\n",
      "rbf kernel\n",
      "accuracy ->  58.08316031648055\n",
      "F1 score -> 0.42682009969891394\n",
      "Polynomial kernel\n",
      "accuracy ->  58.08316031648055\n",
      "F1 score -> 0.42682009969891394\n",
      "Prediction with features combined.... \n",
      "linear kernel \n",
      "accuracy ->  94.41669939958476\n",
      "F1 score -> 0.9440887171600834\n",
      "rbf kernel\n",
      "accuracy ->  58.08316031648055\n",
      "F1 score -> 0.42682009969891394\n",
      "Polynomial kernel\n",
      "accuracy ->  58.08316031648055\n",
      "F1 score -> 0.42682009969891394\n"
     ]
    }
   ],
   "source": [
    "#prediction and accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#word n-grams\n",
    "\n",
    "word_pre = word_clf.predict(word_x_test)\n",
    "word_pre_pre= word_clf_pre.predict(word_x_test)\n",
    "word_pre_poly= word_clf_poly.predict(word_x_test)\n",
    "print('Prediction with word n-gram features .... ')\n",
    "print(\"linear kernel \")\n",
    "print(\"accuracy -> \",accuracy_score(word_pre_pre, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,word_pre_pre,average='weighted'))\n",
    "print(\"rbf kernel\")\n",
    "print(\"accuracy -> \",accuracy_score(word_pre, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,word_pre,average='weighted'))\n",
    "print(\"Polynomial kernel\")\n",
    "print(\"accuracy -> \",accuracy_score(word_pre_poly, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,word_pre_poly,average='weighted'))\n",
    "\n",
    "\n",
    "\n",
    "#char n-grams\n",
    "\n",
    "char_pre = char_clf.predict(char_x_test)\n",
    "char_pre_pre= char_clf_pre.predict(char_x_test)\n",
    "char_pre_poly= char_clf_poly.predict(char_x_test)\n",
    "print('Prediction with char n-gram features .... ')\n",
    "print(\"linear kernel \")\n",
    "print(\"accuracy -> \",accuracy_score(char_pre_pre, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,char_pre_pre,average='weighted'))\n",
    "print(\"rbf kernel\")\n",
    "print(\"accuracy -> \",accuracy_score(char_pre, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,char_pre,average='weighted'))\n",
    "print(\"Polynomial kernel\")\n",
    "print(\"accuracy -> \",accuracy_score(char_pre_poly, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,char_pre_poly,average='weighted'))\n",
    "\n",
    "\n",
    "\n",
    "#combined fea. \n",
    "pre = clf.predict(x_test)\n",
    "pre_pre= clf_pre.predict(x_test)\n",
    "pre_poly= clf_poly.predict(x_test)\n",
    "print('Prediction with features combined.... ')\n",
    "print(\"linear kernel \")\n",
    "print(\"accuracy -> \",accuracy_score(pre_pre, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,pre_pre,average='weighted'))\n",
    "print(\"rbf kernel\")\n",
    "print(\"accuracy -> \",accuracy_score(pre, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,pre,average='weighted'))\n",
    "print(\"Polynomial kernel\")\n",
    "print(\"accuracy -> \",accuracy_score(pre_poly, test_y)*100)\n",
    "print(\"F1 score ->\",f1_score(test_y,pre_poly,average='weighted'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive bayes Accuracy Score ->  90.35714285714286\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes \n",
    "from sklearn import naive_bayes\n",
    "\n",
    "\n",
    "nb = naive_bayes.MultinomialNB().fit(x,train_y)\n",
    "pre = nb.predict(x_test)\n",
    "print(\"Naive bayes Accuracy Score -> \",accuracy_score(pre, test_y)*100)\n",
    "#print(\"F1 score ->\",f1_score(test_y,pre,average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
